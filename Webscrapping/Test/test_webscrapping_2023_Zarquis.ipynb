{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\anaconda\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in d:\\anaconda\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\anaconda\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: openpyxl in d:\\anaconda\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in d:\\anaconda\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\anaconda\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\anaconda\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Collecting newspaper3k\n",
      "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in d:\\anaconda\\lib\\site-packages (from newspaper3k) (4.12.3)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in d:\\anaconda\\lib\\site-packages (from newspaper3k) (10.4.0)\n",
      "Requirement already satisfied: PyYAML>=3.11 in d:\\anaconda\\lib\\site-packages (from newspaper3k) (6.0.1)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in d:\\anaconda\\lib\\site-packages (from newspaper3k) (1.2.0)\n",
      "Requirement already satisfied: lxml>=3.6.0 in d:\\anaconda\\lib\\site-packages (from newspaper3k) (5.2.1)\n",
      "Requirement already satisfied: nltk>=3.2.1 in d:\\anaconda\\lib\\site-packages (from newspaper3k) (3.9.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in d:\\anaconda\\lib\\site-packages (from newspaper3k) (2.32.3)\n",
      "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
      "  Downloading feedparser-6.0.12-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in d:\\anaconda\\lib\\site-packages (from newspaper3k) (5.1.2)\n",
      "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
      "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
      "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
      "     ---------------------------------------- 0.0/7.4 MB ? eta -:--:--\n",
      "     ------------ --------------------------- 2.4/7.4 MB 12.2 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 5.0/7.4 MB 12.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  7.3/7.4 MB 11.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 7.4/7.4 MB 10.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in d:\\anaconda\\lib\\site-packages (from newspaper3k) (2.9.0.post0)\n",
      "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
      "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\anaconda\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.5)\n",
      "Requirement already satisfied: six in d:\\anaconda\\lib\\site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
      "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: click in d:\\anaconda\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
      "Requirement already satisfied: joblib in d:\\anaconda\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\anaconda\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (4.66.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2024.8.30)\n",
      "Requirement already satisfied: requests-file>=1.4 in d:\\anaconda\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in d:\\anaconda\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (3.13.1)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from click->nltk>=3.2.1->newspaper3k) (0.4.6)\n",
      "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
      "Downloading feedparser-6.0.12-py3-none-any.whl (81 kB)\n",
      "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
      "  Building wheel for tinysegmenter (setup.py): started\n",
      "  Building wheel for tinysegmenter (setup.py): finished with status 'done'\n",
      "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13566 sha256=5e25b2b532ebb69a45ec938d6d677d97abf8b955817ec95b6f4b0ec2184ab76e\n",
      "  Stored in directory: c:\\users\\usuario\\appdata\\local\\pip\\cache\\wheels\\a5\\91\\9f\\00d66475960891a64867914273fcaf78df6cb04d905b104a2a\n",
      "  Building wheel for feedfinder2 (setup.py): started\n",
      "  Building wheel for feedfinder2 (setup.py): finished with status 'done'\n",
      "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3358 sha256=507499fa99d8f41b03d9d875e517b9e89d3fab0d8691375f6e0b7ecf46ce34b3\n",
      "  Stored in directory: c:\\users\\usuario\\appdata\\local\\pip\\cache\\wheels\\9f\\9f\\fb\\364871d7426d3cdd4d293dcf7e53d97f160c508b2ccf00cc79\n",
      "  Building wheel for jieba3k (setup.py): started\n",
      "  Building wheel for jieba3k (setup.py): finished with status 'done'\n",
      "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398386 sha256=4034fac36f54874ab8938248ed07f926ebe72043574319f17d2d33eac7f9f45e\n",
      "  Stored in directory: c:\\users\\usuario\\appdata\\local\\pip\\cache\\wheels\\26\\72\\f7\\fff392a8d4ea988dea4ccf9788599d09462a7f5e51e04f8a92\n",
      "  Building wheel for sgmllib3k (setup.py): started\n",
      "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6060 sha256=863b77bd41e6ac2046b24b808e7532cb467ff2c4d26ec48a0678f47c7d6767e4\n",
      "  Stored in directory: c:\\users\\usuario\\appdata\\local\\pip\\cache\\wheels\\03\\f5\\1a\\23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
      "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
      "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, feedfinder2, newspaper3k\n",
      "Successfully installed feedfinder2-0.0.4 feedparser-6.0.12 jieba3k-0.35.1 newspaper3k-0.2.8 sgmllib3k-1.0.0 tinysegmenter-0.3\n",
      "Collecting selenium\n",
      "  Downloading selenium-4.36.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting urllib3<3.0,>=2.5.0 (from urllib3[socks]<3.0,>=2.5.0->selenium)\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting trio<1.0,>=0.30.0 (from selenium)\n",
      "  Downloading trio-0.31.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket<1.0,>=0.12.2 (from selenium)\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting certifi>=2025.6.15 (from selenium)\n",
      "  Downloading certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting typing_extensions<5.0,>=4.14.0 (from selenium)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in d:\\anaconda\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Collecting attrs>=23.2.0 (from trio<1.0,>=0.30.0->selenium)\n",
      "  Downloading attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: sortedcontainers in d:\\anaconda\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in d:\\anaconda\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (3.7)\n",
      "Collecting outcome (from trio<1.0,>=0.30.0->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in d:\\anaconda\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in d:\\anaconda\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (1.17.1)\n",
      "Collecting wsproto>=0.14 (from trio-websocket<1.0,>=0.12.2->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in d:\\anaconda\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in d:\\anaconda\\lib\\site-packages (from cffi>=1.14->trio<1.0,>=0.30.0->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in d:\\anaconda\\lib\\site-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.14.0)\n",
      "Downloading selenium-4.36.0-py3-none-any.whl (9.6 MB)\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 2.4/9.6 MB 12.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.7/9.6 MB 11.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.3/9.6 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.4/9.6 MB 12.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.6/9.6 MB 9.6 MB/s eta 0:00:00\n",
      "Downloading certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
      "Downloading trio-0.31.0-py3-none-any.whl (512 kB)\n",
      "Downloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Downloading attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: wsproto, urllib3, typing_extensions, certifi, attrs, outcome, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.3\n",
      "    Uninstalling urllib3-2.2.3:\n",
      "      Successfully uninstalled urllib3-2.2.3\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2024.8.30\n",
      "    Uninstalling certifi-2024.8.30:\n",
      "      Successfully uninstalled certifi-2024.8.30\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "Successfully installed attrs-25.4.0 certifi-2025.10.5 outcome-1.3.0.post0 selenium-4.36.0 trio-0.31.0 trio-websocket-0.12.2 typing_extensions-4.15.0 urllib3-2.5.0 wsproto-1.2.0\n",
      "Collecting openai\n",
      "  Downloading openai-2.3.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\anaconda\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\anaconda\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\anaconda\\lib\\site-packages (from openai) (0.27.0)\n",
      "Collecting jiter<1,>=0.10.0 (from openai)\n",
      "  Downloading jiter-0.11.0-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in d:\\anaconda\\lib\\site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in d:\\anaconda\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in d:\\anaconda\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in d:\\anaconda\\lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\anaconda\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in d:\\anaconda\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\anaconda\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\anaconda\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\anaconda\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in d:\\anaconda\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-2.3.0-py3-none-any.whl (999 kB)\n",
      "   ---------------------------------------- 0.0/999.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 999.8/999.8 kB 7.9 MB/s eta 0:00:00\n",
      "Downloading jiter-0.11.0-cp312-cp312-win_amd64.whl (203 kB)\n",
      "Installing collected packages: jiter, openai\n",
      "Successfully installed jiter-0.11.0 openai-2.3.0\n",
      "Collecting serpapi\n",
      "  Downloading serpapi-0.1.5-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (from serpapi) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests->serpapi) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests->serpapi) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests->serpapi) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests->serpapi) (2025.10.5)\n",
      "Downloading serpapi-0.1.5-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: serpapi\n",
      "Successfully installed serpapi-0.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install openpyxl\n",
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "!pip install newspaper3k\n",
    "!pip install selenium\n",
    "!pip install openai\n",
    "!pip install serpapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install googlesearch-python\n",
    "!pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-search-results in /opt/anaconda3/lib/python3.11/site-packages (2.4.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (from google-search-results) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests->google-search-results) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests->google-search-results) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests->google-search-results) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests->google-search-results) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"httpx[http2]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchrome\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Options  \u001b[38;5;66;03m# Configurar opciones para Chrome (modo sin cabeza, user-agent, etc.)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m  \u001b[38;5;66;03m# Cliente oficial para acceder a la API de OpenAI (ChatGPT, GPT-4, etc.)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokens_por_posicion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m token_at\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Librer√≠as est√°ndar (de Python) ---\n",
    "import os  # Acceso a funcionalidades del sistema operativo (rutas, variables de entorno, etc.)\n",
    "import re  # Permite trabajar con expresiones regulares (b√∫squeda de patrones en texto)\n",
    "import time  # Funciones relacionadas con el tiempo (pausas, medir duraci√≥n, timestamps)\n",
    "import json  # Para trabajar con archivos y datos en formato JSON\n",
    "import tempfile  # Crear archivos/directorios temporales (√∫til para manejo temporal de datos)\n",
    "from typing import Dict, List  # Anotaciones de tipo para funciones (mejor legibilidad y autocompletado)\n",
    "from pathlib import Path  # Manejo de rutas de archivos y carpetas de forma m√°s moderna y robusta\n",
    "from GoogleNews import GoogleNews\n",
    "\n",
    "# --- Librer√≠as externas (instaladas con pip) ---\n",
    "import pandas as pd  # An√°lisis y manipulaci√≥n de datos tabulares (Excel, CSV, DataFrames, etc.)\n",
    "\n",
    "import openpyxl  # Librer√≠a para leer y escribir archivos Excel (.xlsx)\n",
    "from openpyxl import load_workbook  # Cargar y modificar libros de Excel ya existentes\n",
    "\n",
    "import requests  # Realizar peticiones HTTP (GET, POST, etc.) para acceder a contenido web o APIs\n",
    "from urllib.parse import urlparse, parse_qs, unquote\n",
    "from bs4 import BeautifulSoup  # Analizar y extraer informaci√≥n de HTML (web scraping)\n",
    "\n",
    "from newspaper import Article  # Extraer autom√°ticamente informaci√≥n estructurada de art√≠culos web (t√≠tulo, texto, etc.)\n",
    "\n",
    "from selenium import webdriver  # Controlar un navegador web desde Python (automatizaci√≥n para scraping din√°mico)\n",
    "from selenium.webdriver.chrome.options import Options  # Configurar opciones para Chrome (modo sin cabeza, user-agent, etc.)\n",
    "\n",
    "import openai  # Cliente oficial para acceder a la API de OpenAI (ChatGPT, GPT-4, etc.)\n",
    "\n",
    "from src.tokens_por_posicion import token_at\n",
    "\n",
    "#from serpapi import GoogleSearch  # Realizar b√∫squedas en Google a trav√©s de la API de SerpAPI (scraping sin bloquearse)\n",
    "\n",
    "# --- M√≥dulos locales del proyecto ---\n",
    "#from src.functions import completar, _debug_api_key_openai\n",
    "# completar: funci√≥n personalizada (probablemente genera texto con GPT u otra IA)\n",
    "# _debug_api_key_openai: funci√≥n que permite verificar que la clave de OpenAI est√° bien cargada\n",
    "\n",
    "#from src.tokens_por_posicion import token_at\n",
    "# token_at: funci√≥n propia para extraer tokens desde una posici√≥n espec√≠fica (seguramente NLP)\n",
    "\n",
    "#from secrets_local import TOKENS\n",
    "# TOKENS: contiene claves o tokens de autenticaci√≥n (como API keys privadas). ¬°Nunca subir este archivo a repos p√∫blicos!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            query  \\\n",
      "0  \"paro de docentes\" Bogot√° \"clases suspendidas\"   \n",
      "1  \"paro de docentes\" Bogot√° \"clases suspendidas\"   \n",
      "2  \"paro de docentes\" Bogot√° \"clases suspendidas\"   \n",
      "3  \"paro de docentes\" Bogot√° \"clases suspendidas\"   \n",
      "4  \"paro de docentes\" Bogot√° \"clases suspendidas\"   \n",
      "\n",
      "                                               title               media  \\\n",
      "0  Soacha suspende clases presenciales por paro d...  Noticias D√≠a a D√≠a   \n",
      "1  Suspenden clases presenciales en Soacha por pa...    Soacha Ilustrada   \n",
      "2  Suspensi√≥n de clases presenciales en instituci...  Alcaldia de Soacha   \n",
      "3  Colegios oficiales de Soacha suspenden clases ...    Noticias Caracol   \n",
      "4  Soacha suspende clases presenciales por paro d...       Q¬¥hubo Bogot√°   \n",
      "\n",
      "         date                                               link  \n",
      "0  Hace 1 mes  https://www.noticiasdiaadia.com/cundinamarca/s...  \n",
      "1  Hace 1 mes  https://soachailustrada.com/59317/&ved=2ahUKEw...  \n",
      "2  Hace 1 mes  https://www.alcaldiasoacha.gov.co/Noticias/Pag...  \n",
      "3  Hace 1 mes  https://www.noticiascaracol.com/colombia/coleg...  \n",
      "4  Hace 1 mes  https://www.qhubobogota.com/asi-paso/soacha-su...  \n",
      "üìÅ Resultados guardados en paros_docentes_bogota_2023.csv\n"
     ]
    }
   ],
   "source": [
    "from GoogleNews import GoogleNews\n",
    "import pandas as pd\n",
    "\n",
    "# Inicializamos GoogleNews en espa√±ol\n",
    "googlenews = GoogleNews(lang='es', encode='utf-8')\n",
    "\n",
    "# Establecer rango de fechas: todo 2023\n",
    "googlenews.set_time_range('01/01/2023', '12/31/2023')\n",
    "\n",
    "# Consultas espec√≠ficas\n",
    "queries = [\n",
    "    '\"paro de docentes\" Bogot√° \"clases suspendidas\"',\n",
    "    '(paro OR \"cese de actividades\") AND (docentes OR maestros) AND Bogot√°',\n",
    "    '\"estudiantes sin clase\" AND paro Bogot√°',\n",
    "    '\"suspensi√≥n de clases\" AND docentes Bogot√°'\n",
    "]\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for q in queries:\n",
    "    googlenews.search(q)\n",
    "    results = googlenews.result()\n",
    "    for r in results:\n",
    "        all_results.append({\n",
    "            'query': q,\n",
    "            'title': r.get('title'),\n",
    "            'media': r.get('media'),\n",
    "            'date': r.get('date'),\n",
    "            'link': r.get('link')\n",
    "        })\n",
    "\n",
    "# Guardar en DataFrame\n",
    "df = pd.DataFrame(all_results)\n",
    "print(df.head())\n",
    "\n",
    "# Exportar a CSV\n",
    "df.to_csv(\"paros_docentes_bogota_2023.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"üìÅ Resultados guardados en paros_docentes_bogota_2023.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serp API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from serpapi.google_search import GoogleSearch\n",
    "import pandas as pd\n",
    "SerpAPI_tkn1 = '0a6f749d6af6ba0055e057b875da0b68d4938d4027e6eb27e34c2aff0f81c59f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Iniciando b√∫squeda en SerpApi con consulta simple...\n",
      "‚úÖ B√∫squeda completada.\n",
      "\n",
      "--- ¬°Resultados Encontrados! ---\n",
      "\n",
      "                                               title  \\\n",
      "0  Paro nacional de profesores, 30 de agosto de 2...   \n",
      "1  Los maestros en Colombia no podr√≠an volver a paro   \n",
      "2  Protestas en Colombia Defensor√≠a reporta aumen...   \n",
      "3                      CIRCULAR No. 004 de 2023 PARA   \n",
      "4  RPC Radio | El @meducapma inform√≥ la suspensi√≥...   \n",
      "5  Protesta de profesores en Bogot√°: as√≠ avanza l...   \n",
      "6       Marchas HOY 28 de marzo Bogot√°: Minuto a ...   \n",
      "7  Conflictos sociales en Colombia: 34 eventos se...   \n",
      "8  Sala Laboral se√±ala que no puede haber una pro...   \n",
      "9  Concepto 217971 de 2023 Departamento Administr...   \n",
      "\n",
      "                                                link  \\\n",
      "0  https://www.elpais.com.co/colombia/paro-nacion...   \n",
      "1  https://www.infobae.com/colombia/2023/08/02/pa...   \n",
      "2  https://www.eltiempo.com/justicia/investigacio...   \n",
      "3  https://www.educacionbogota.edu.co/portal_inst...   \n",
      "4           https://www.instagram.com/p/CyzPDt9tg_j/   \n",
      "5  https://www.alertabogota.com/noticias/local/pr...   \n",
      "6  https://caracol.com.co/2023/03/28/marchas-hoy-...   \n",
      "7  https://www.infobae.com/colombia/2023/09/28/co...   \n",
      "8  https://cortesuprema.gov.co/sala-laboral-senal...   \n",
      "9  https://www.funcionpublica.gov.co/eva/gestorno...   \n",
      "\n",
      "                                             snippet  \n",
      "0  Las protestas saldr√°n desde las 9:30 a.m., el ...  \n",
      "1  ‚ÄúEmpezaron en 2018 con un cese de actividades ...  \n",
      "2  ... cese de actividades (4 por ciento); y en e...  \n",
      "3  Las actividades programadas previamente en el ...  \n",
      "4  El @meducapma inform√≥ la suspensi√≥n de clases ...  \n",
      "5  Los docentes hacen un cese de actividades de d...  \n",
      "6  Este martes 28 de marzo, la Asociaci√≥n Distrit...  \n",
      "7  Marchas y movilizaciones. Paros c√≠vicos. Cese ...  \n",
      "8  Bogot√°, D.C., martes 24 de octubre de 2023. Pe...  \n",
      "9  ... cese de actividades en la alcald√≠a? le ind...  \n",
      "\n",
      "------------------------------\n",
      "\n",
      "üìÅ Resultados guardados en test_serpapi_simple_resultados.csv\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    '(\"paro docentes\" OR \"suspensi√≥n de clases\" OR \"cese de actividades\") '\n",
    "    'Bogot√° 2023'\n",
    ")\n",
    "\n",
    "# ‚öôÔ∏è Par√°metros de la B√∫squeda\n",
    "params = {\n",
    "    \"engine\": \"google\",\n",
    "    # Mantenemos el filtro de fecha de 2023\n",
    "    \"q\": f\"{query} after:2023-01-01 before:2023-12-31\", \n",
    "    \"api_key\": SerpAPI_tkn1,\n",
    "    \"num\": 50   # Pedimos 10 resultados\n",
    "}\n",
    "\n",
    "# üöÄ Ejecutar la B√∫squeda\n",
    "print(\"üîç Iniciando b√∫squeda en SerpApi con consulta simple...\")\n",
    "try:\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "    print(\"‚úÖ B√∫squeda completada.\")\n",
    "\n",
    "    # üìä Procesar Resultados Org√°nicos\n",
    "    items = results.get(\"organic_results\", [])\n",
    "    df = pd.DataFrame(items)\n",
    "\n",
    "    # üíæ Guardar y Mostrar\n",
    "    if not df.empty:\n",
    "        # Seleccionar solo las columnas de inter√©s\n",
    "        df = df[[\"title\", \"link\", \"snippet\"]]\n",
    "        \n",
    "        print(\"\\n--- ¬°Resultados Encontrados! ---\\n\")\n",
    "        print(df)\n",
    "        \n",
    "        # Guardar en CSV\n",
    "        df.to_csv(\"test_serpapi_simple_resultados.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "        print(\"\\n------------------------------\\n\")\n",
    "        print(\"üìÅ Resultados guardados en test_serpapi_simple_resultados.csv\")\n",
    "    else:\n",
    "        print(\"\\n------------------------------\\n\")\n",
    "        print(\"‚ö†Ô∏è No se encontraron resultados. El problema podr√≠a ser la clave API o el saldo.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Ocurri√≥ un error al ejecutar la b√∫squeda: {e}\")\n",
    "    print(\"Esto casi siempre indica una clave API incorrecta, sin saldo o un problema de red.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (230884762.py, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[102], line 21\u001b[0;36m\u001b[0m\n\u001b[0;31m    except Exception as e:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from serpapi.google_search import GoogleSearch\n",
    "\n",
    "# Reemplaza con tu propia API key\n",
    "api_key = \"3473fc2d447c6d1244dd8112d79b8d89fb79aa7a\"\n",
    "\n",
    "# Par√°metros para la b√∫squeda\n",
    "params = {\n",
    "    \"engine\": \"google\",\n",
    "    \"q\": \"paros docentes en 2023\",\n",
    "    \"location\": \"Colombia\",  # Cambia si quieres otra regi√≥n\n",
    "    \"hl\": \"es\",               # Idioma espa√±ol\n",
    "    \"gl\": \"col\",               # Resultados de Google Argentina\n",
    "    \"api_key\": api_key,\n",
    "}\n",
    "\n",
    "# Realizar la b√∫squeda\n",
    "search = GoogleSearch(params)\n",
    "results = search.get_dict()\n",
    "\n",
    "# Extraer resultados org√°nicos (pueden incluir noticias o p√°ginas web)\n",
    "for i, result in enumerate(results.get(\"organic_results\", []), start=1):\n",
    "    title = result.get(\"title\")\n",
    "    link = result.get(\"link\")\n",
    "    snippet = result.get(\"snippet\", \"\")\n",
    "    print(f\"{i}. {title}\\n   {link}\\n   {snippet}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"error\": \"Invalid API key. Your API key should be here: https://serpapi.com/manage-api-key\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from serpapi.google_search import GoogleSearch\n",
    "\n",
    "params = {\n",
    "    \"engine\": \"google\",\n",
    "    \"q\": \"paro docente 2023\",\n",
    "    \"api_key\": \"3473fc2d447c6d1244dd8112d79b8d89fb79aa7a\"\n",
    "}\n",
    "\n",
    "search = GoogleSearch(params)\n",
    "results = search.get_dict()\n",
    "\n",
    "# Mostrar todo lo que devuelve la API\n",
    "import json\n",
    "print(json.dumps(results, indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
