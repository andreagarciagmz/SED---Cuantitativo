{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lMF9PrGneH1b",
        "outputId": "34fba6c7-3fe1-45e5-a0ea-49cb67911603"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Collecting datetime\n",
            "  Downloading DateTime-5.5-py3-none-any.whl.metadata (33 kB)\n",
            "Collecting zope.interface (from datetime)\n",
            "  Downloading zope_interface-8.0.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from datetime) (2025.2)\n",
            "Downloading DateTime-5.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zope_interface-8.0.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (264 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.7/264.7 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: zope.interface, datetime\n",
            "Successfully installed datetime-5.5 zope.interface-8.0.1\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from bs4) (4.13.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->bs4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->bs4) (4.15.0)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.2\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement json (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for json\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip  install pandas\n",
        "!pip  install numpy\n",
        "!pip  install datetime\n",
        "!pip  install regex\n",
        "!pip  install openai\n",
        "!pip  install bs4\n",
        "!pip  install requests\n",
        "!pip  install json\n",
        "!pip  install numpy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6hUuh6OW9cwQ",
        "outputId": "235c613c-615f-4268-defb-9c27daf91556"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noticias filtradas: 0\n",
            "Empty DataFrame\n",
            "Columns: [SQLDATE, ActionGeo_FullName, SOURCEURL]\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ==============================\n",
        "# 1. Cargar archivo CSV\n",
        "# ==============================\n",
        "df = pd.read_csv(\"bq-results-20251023-000630-1761178021663.csv\", encoding=\"utf-8-sig\")\n",
        "\n",
        "# Limpieza de encabezados para evitar errores por espacios o caracteres invisibles\n",
        "df.columns = (\n",
        "    df.columns.str.encode('utf-8').str.decode('utf-8', 'ignore')\n",
        "    .str.replace(\"ï»¿\", \"\", regex=False)\n",
        "    .str.strip()\n",
        ")\n",
        "\n",
        "# ==============================\n",
        "# 2. Limpieza y formato de fecha\n",
        "# ==============================\n",
        "# Verificar si la columna SQLDATE existe\n",
        "col_fecha = [c for c in df.columns if \"SQLDATE\" in c.upper()]\n",
        "if not col_fecha:\n",
        "    raise ValueError(\"No se encontró la columna SQLDATE en el archivo CSV.\")\n",
        "col_fecha = col_fecha[0]\n",
        "\n",
        "# Convertir fechas con detección automática\n",
        "df[col_fecha] = pd.to_datetime(df[col_fecha].astype(str), errors='coerce')\n",
        "df_2013 = df[df[col_fecha].dt.year == 2013].copy()\n",
        "\n",
        "# ==============================\n",
        "# 4. Filtro por país Colombia\n",
        "# ==============================\n",
        "col_pais = [c for c in df.columns if \"COUNTRYCODE\" in c.upper()]\n",
        "if not col_pais:\n",
        "    raise ValueError(\"No se encontró la columna ActionGeo_CountryCode en el archivo CSV.\")\n",
        "col_pais = col_pais[0]\n",
        "\n",
        "df_colombia = df_2013[df_2013[col_pais].astype(str).str.upper() == \"CO\"].copy()\n",
        "\n",
        "# ==============================\n",
        "# 5. Filtro por contenido del SOURCEURL\n",
        "# ==============================\n",
        "col_url = [c for c in df.columns if \"SOURCEURL\" in c.upper()]\n",
        "if not col_url:\n",
        "    raise ValueError(\"No se encontró la columna SOURCEURL en el archivo CSV.\")\n",
        "col_url = col_url[0]\n",
        "\n",
        "df_colombia[col_url] = df_colombia[col_url].astype(str).str.lower()\n",
        "\n",
        "palabras_clave = ['paro-docente', 'fecode', 'maestros', 'protesta-docente', 'paro-maestros']\n",
        "\n",
        "filtro_tema = df_colombia[col_url].apply(\n",
        "    lambda x: any(palabra in x for palabra in palabras_clave)\n",
        ")\n",
        "\n",
        "df_filtrado = df_colombia.loc[filtro_tema, [col_fecha, 'ActionGeo_FullName', col_url]].copy()\n",
        "\n",
        "# ==============================\n",
        "# 6. Eliminar duplicados\n",
        "# ==============================\n",
        "df_filtrado = df_filtrado.drop_duplicates(subset=[col_url], keep='first')\n",
        "\n",
        "# ==============================\n",
        "# 7. Guardar resultado final\n",
        "# ==============================\n",
        "print(\"Noticias filtradas:\", len(df_filtrado))\n",
        "print(df_filtrado.head())\n",
        "\n",
        "df_filtrado.to_csv(\"BQ_Noticias_Paro_Docente_2013.csv\", index=False, encoding=\"utf-8-sig\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nyPkGByi3Cot"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# === 1. Cargar el archivo ===\n",
        "df = pd.read_csv(\"Base-SGI-suspensión-clases-2012-2025-03_08_2025_VALIDACION-SED.csv\")\n",
        "\n",
        "# Normalizar nombres de columnas (elimina espacios, mayúsculas y tildes)\n",
        "df.columns = df.columns.str.strip().str.lower().str.replace(\"á\", \"a\").str.replace(\"é\", \"e\").str.replace(\"í\", \"i\").str.replace(\"ó\", \"o\").str.replace(\"ú\", \"u\")\n",
        "\n",
        "# === 2. Unificar formato de fechas ===\n",
        "for col in df.columns:\n",
        "    if \"fecha\" in col:\n",
        "        df[col] = pd.to_datetime(df[col], errors=\"coerce\", dayfirst=True)\n",
        "\n",
        "# === 3. Filtrar por año  2015  ===\n",
        "df_2015 = df[\n",
        "    (df[\"fecha inicio\"].dt.year == 2013) |\n",
        "    (df[\"fecha fin\"].dt.year == 2013)\n",
        "]\n",
        "\n",
        "\n",
        "# Seleccionar las más relevantes si existen\n",
        "cols_posibles = [\n",
        "    \"fecha inicio\", \"fecha fin\", \"convocante\", \"hecho\",\"dias suspension clases\",\n",
        "    \"ubicacion\", \"medio de verificacion\",\"medio de verificacion 2\", \"tipo de protesta\", \"alcance\"\n",
        "]\n",
        "cols_finales = [c for c in cols_posibles if c in df_2015.columns]\n",
        "\n",
        "df_resultado = df_2015[cols_finales].reset_index(drop=True)\n",
        "if \"medio de verificacion\" in df_resultado.columns:\n",
        "    sin_info = df_resultado[df_resultado[\"medio de verificacion\"].str.strip().str.upper() == \"S/I\"]\n",
        "    con_info = df_resultado[df_resultado[\"medio de verificacion\"].str.strip().str.upper() != \"S/I\"]\n",
        "    con_info = con_info.drop_duplicates(subset=[\"medio de verificacion\"], keep='first')\n",
        "    df_resultado = pd.concat([con_info, sin_info], ignore_index=True)\n",
        "\n",
        "# === 8. (Opcional) Guardar en CSV ===\n",
        "df_resultado.to_csv(\"SED_Paros_Docentes_Colombia_2013.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# === 1. Cargar archivo ===\n",
        "df = pd.read_csv(\"web_noticias_paros_docentes_2013.csv\")\n",
        "\n",
        "# === 2. Estandarizar nombres de columnas ===\n",
        "df.columns = (\n",
        "    df.columns\n",
        "    .str.strip()\n",
        "    .str.lower()\n",
        "    .str.replace(\"á\", \"a\")\n",
        "    .str.replace(\"é\", \"e\")\n",
        "    .str.replace(\"í\", \"i\")\n",
        "    .str.replace(\"ó\", \"o\")\n",
        "    .str.replace(\"ú\", \"u\")\n",
        ")\n",
        "\n",
        "# === 3. Convertir la columna de fecha_publicacion a formato fecha y usarla como fecha_paro ===\n",
        "df[\"fecha_publicacion\"] = pd.to_datetime(df[\"fecha_publicacion\"], errors=\"coerce\", dayfirst=True)\n",
        "df[\"fecha_paro\"] = df[\"fecha_publicacion\"]  # Usar fecha_publicacion como fecha_paro\n",
        "\n",
        "# === 4. Filtrar solo los paros del año 2013 ===\n",
        "df_2013 = df[df[\"fecha_paro\"].dt.year == 2013]\n",
        "\n",
        "# === 5. Seleccionar columnas relevantes ===\n",
        "cols_finales = [\n",
        "    \"titulo\",\n",
        "    \"fuente\",\n",
        "    \"url\",\n",
        "    \"fecha_paro\",\n",
        "    \"duracion_dias\",\n",
        "    \"organizaciones_sindicales\",\n",
        "    \"razones_paro\",\n",
        "    \"resumen\",\n",
        "    \"ubicacion\",\n",
        "    \"tipo_movilizacion\"\n",
        "]\n",
        "\n",
        "df_reducido = df_2013[cols_finales].reset_index(drop=True)\n",
        "\n",
        "# === 6. Exportar a CSV ===\n",
        "df_reducido.to_csv(\"WEB_Paros_Docentes_2013_Reducido.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSn0e9F2C_BO",
        "outputId": "bf0c2fe8-38ce-4f05-ea4c-2cde82d2a7bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2875457156.py:19: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
            "  df[\"fecha_publicacion\"] = pd.to_datetime(df[\"fecha_publicacion\"], errors=\"coerce\", dayfirst=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QMDMrbFMdYVI",
        "outputId": "da4c2c4b-a060-4792-bab4-19e38d4bfead"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement os (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for os\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement json (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for json\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: bs4 in /usr/local/lib/python3.12/dist-packages (0.0.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from bs4) (4.13.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->bs4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->bs4) (4.15.0)\n",
            "Requirement already satisfied: datetime in /usr/local/lib/python3.12/dist-packages (5.5)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.12/dist-packages (from datetime) (8.0.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from datetime) (2025.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement time (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for time\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement urllib.parse (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for urllib.parse\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement difflib (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for difflib\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install os\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install requests\n",
        "!pip install json\n",
        "!pip install bs4\n",
        "!pip install datetime\n",
        "!pip install time\n",
        "!pip install urllib.parse\n",
        "!pip install difflib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGUYefg2c2WO"
      },
      "outputs": [],
      "source": [
        "import os, re\n",
        "import pandas as pd, numpy as np, requests, json, time\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "from openai import OpenAI\n",
        "from urllib.parse import urlparse, parse_qsl, urlencode, urlunparse\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrbHGmGi8XnK",
        "outputId": "b5dfad4d-6f31-459a-dc70-080e146de341"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ BASE_FINAL_PAROS_DOCENTES_2023_VALIDADA.csv generada correctamente\n",
            "\n",
            "Resumen general:\n",
            "- Total_paros: 11\n",
            "- Promedio_duracion: 6.0\n",
            "- Max_duracion: 18.0\n",
            "- Min_duracion: 1.0\n",
            "- Tipos_movilizacion: {'paro docente': 6, 'Paro': 3, 'Sin evidencia': 1, 'Movilización contenida en paro': 1}\n",
            "- Ciudades_principales: {'Colombia': 6, '': 3, 'Secretaría de Educación (Av. El Dorado) y finalizó frente al Ministerio de Educa': 1, 'Bogotá': 1}\n",
            "- Sindicatos_principales: {'ADE | FECODE': 5, 'NO ESPECÍFICA': 2, 'CUT | FECODE': 2, 'FECODE': 1, 'SUTEV': 1}\n",
            "- Fuentes: {'WEB': 6, 'SED': 5}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-605807243.py:206: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  base = pd.concat([sed_final[cols_union], web_final[cols_union], bq_final[cols_union]], ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "FILE_WEB = \"WEB_Paros_Docentes_2013_Reducido.csv\"\n",
        "FILE_SED = \"SED_Paros_Docentes_Colombia_2013.csv\"\n",
        "FILE_BQ  = \"BQ_Noticias_Paro_Docente_2013.csv\"\n",
        "\n",
        "# -------------------- UTILIDADES --------------------\n",
        "def norm_cols(df):\n",
        "    df.columns = (df.columns.str.strip().str.lower()\n",
        "                  .str.replace(\"á\",\"a\").str.replace(\"é\",\"e\")\n",
        "                  .str.replace(\"í\",\"i\").str.replace(\"ó\",\"o\").str.replace(\"ú\",\"u\"))\n",
        "    return df\n",
        "\n",
        "def safe_dt(x, dayfirst=False):\n",
        "    return pd.to_datetime(x, errors=\"coerce\", dayfirst=dayfirst)\n",
        "\n",
        "def get_col(df, keys):\n",
        "    for k in keys:\n",
        "        hits = [c for c in df.columns if k in c]\n",
        "        if hits: return hits[0]\n",
        "    return None\n",
        "\n",
        "def ensure_cols(df, cols, fill=np.nan):\n",
        "    for c in cols:\n",
        "        if c not in df.columns:\n",
        "            df[c] = fill\n",
        "    return df\n",
        "\n",
        "# -------------------- CARGA --------------------\n",
        "web = norm_cols(pd.read_csv(FILE_WEB))\n",
        "sed = norm_cols(pd.read_csv(FILE_SED))\n",
        "# Check if BQ file is empty before loading\n",
        "try:\n",
        "    bq = norm_cols(pd.read_csv(FILE_BQ))\n",
        "except pd.errors.EmptyDataError:\n",
        "    print(f\"Warning: File {FILE_BQ} is empty. Skipping BQ data.\")\n",
        "    bq = pd.DataFrame()\n",
        "\n",
        "\n",
        "for df in [web, sed, bq]:\n",
        "    if not df.empty:\n",
        "        for c in df.columns:\n",
        "            if \"fecha\" in c:\n",
        "                df[c] = safe_dt(df[c], dayfirst=False)\n",
        "\n",
        "# ================================================================\n",
        "# BASE SED\n",
        "# ================================================================\n",
        "col_fecha = get_col(sed, [\"fecha inicio\",\"fechainicio\",\"fecha\"])\n",
        "col_fin   = get_col(sed, [\"fecha fin\",\"fechafin\",\"fin\"])\n",
        "col_hecho = get_col(sed, [\"hecho\",\"evento\",\"descripcion\"])\n",
        "col_tipo  = get_col(sed, [\"tipo de protesta\",\"tipo\",\"movilizacion\"])\n",
        "col_conv  = get_col(sed, [\"convocante\",\"organizacion\",\"sindicato\"])\n",
        "col_ubic  = get_col(sed, [\"ubicacion\",\"lugar\",\"region\"])\n",
        "col_dias  = get_col(sed, [\"dias suspension\",\"duracion_dias\",\"dias\",\"duracion\"])\n",
        "\n",
        "if not col_dias and col_fecha and col_fin:\n",
        "    sed[\"duracion_dias\"] = (safe_dt(sed[col_fin]) - safe_dt(sed[col_fecha])).dt.days + 1\n",
        "    sed.loc[sed[\"duracion_dias\"] < 1, \"duracion_dias\"] = 1\n",
        "    col_dias = \"duracion_dias\"\n",
        "elif not col_dias:\n",
        "    sed[\"duracion_dias\"] = np.nan\n",
        "    col_dias = \"duracion_dias\"\n",
        "\n",
        "sel = [c for c in [col_fecha,col_hecho,col_tipo,col_conv,col_ubic,col_dias] if c]\n",
        "sed_final = sed[sel].rename(columns={\n",
        "    col_fecha:\"fecha_evento\",\n",
        "    col_hecho:\"razones_paro\",\n",
        "    col_tipo:\"tipo_movilizacion\",\n",
        "    col_conv:\"organizaciones_sindicales\",\n",
        "    col_ubic:\"region\",\n",
        "    col_dias:\"duracion_dias\"\n",
        "}).copy()\n",
        "\n",
        "std = [\"fecha_evento\",\"region\",\"organizaciones_sindicales\",\"tipo_movilizacion\",\n",
        "       \"duracion_dias\",\"razones_paro\",\"resumen\",\"titulo_ref\",\"urls\",\"fuentes_presentes\"]\n",
        "sed_final = ensure_cols(sed_final, std)\n",
        "sed_final[\"resumen\"] = sed_final[\"razones_paro\"]\n",
        "sed_final[\"titulo_ref\"] = sed_final[\"razones_paro\"].astype(str).str[:90]\n",
        "sed_final[\"fuentes_presentes\"] = \"SED\"\n",
        "\n",
        "col_url_sed = get_col(sed, [\"medio de verificacion\",\"url\",\"enlace\"])\n",
        "sed_final[\"urls\"] = sed[col_url_sed] if col_url_sed else np.nan\n",
        "\n",
        "# ================================================================\n",
        "#  BASE WEB\n",
        "# ================================================================\n",
        "web_final = web.rename(columns={\"fecha_paro\":\"fecha_evento\",\"ubicacion\":\"region\"}).copy()\n",
        "web_final = ensure_cols(web_final, std)\n",
        "web_final[\"fuentes_presentes\"] = \"WEB\"\n",
        "if \"url\" in web_final.columns: web_final[\"urls\"] = web_final[\"url\"]\n",
        "if \"titulo\" in web_final.columns: web_final[\"titulo_ref\"] = web_final[\"titulo\"]\n",
        "\n",
        "# ================================================================\n",
        "#  BASE BQ\n",
        "# ================================================================\n",
        "bq_final = pd.DataFrame()\n",
        "if not bq.empty:\n",
        "    # Normalizar encabezados\n",
        "    bq.columns = (\n",
        "        bq.columns.str.encode('utf-8').str.decode('utf-8', 'ignore')\n",
        "        .str.replace(\"ï»¿\", \"\", regex=False)\n",
        "        .str.strip()\n",
        "        .str.lower()\n",
        "    )\n",
        "\n",
        "    col_sqldate = [c for c in bq.columns if \"sqldate\" in c]\n",
        "    col_url_bq = [c for c in bq.columns if \"sourceurl\" in c or \"url\" in c]\n",
        "    col_region_bq = [c for c in bq.columns if \"actiongeo_fullname\" in c or \"region\" in c]\n",
        "\n",
        "    if not col_sqldate:\n",
        "        print(\"Warning: No se encontró la columna SQLDATE en la base BQ.\")\n",
        "        bq_final = pd.DataFrame(columns=std) # Create empty df with standard columns\n",
        "    else:\n",
        "        col_sqldate = col_sqldate[0]\n",
        "        col_url_bq = col_url_bq[0] if col_url_bq else None\n",
        "        col_region_bq = col_region_bq[0] if col_region_bq else None\n",
        "\n",
        "        def parse_fecha(f):\n",
        "            if pd.isna(f):\n",
        "                return pd.NaT\n",
        "            f = str(f).strip()\n",
        "            for fmt in (\"%Y%m%d\", \"%d/%m/%Y\", \"%Y-%m-%d\"):\n",
        "                try:\n",
        "                    return pd.to_datetime(f, format=fmt)\n",
        "                except Exception:\n",
        "                    continue\n",
        "            return pd.to_datetime(f, errors=\"coerce\")\n",
        "\n",
        "        bq[\"fecha_evento\"] = bq[col_sqldate].apply(parse_fecha)\n",
        "\n",
        "        # Crear DataFrame final con columnas estandarizadas\n",
        "        bq_final = pd.DataFrame({\n",
        "            \"fecha_evento\": bq[\"fecha_evento\"],\n",
        "            \"urls\": bq[col_url_bq] if col_url_bq else np.nan,\n",
        "            \"region\": bq[col_region_bq] if col_region_bq else np.nan,\n",
        "            \"organizaciones_sindicales\": bq.get(\"organizaciones_sindicales\", np.nan),\n",
        "            \"tipo_movilizacion\": bq.get(\"tipo_movilizacion\", \"paro docente\"),\n",
        "            \"razones_paro\": bq.get(\"razones_paro\", np.nan),\n",
        "            \"resumen\": bq.get(\"resumen\", np.nan),\n",
        "            \"duracion_dias\": bq.get(\"duracion_dias\", np.nan),\n",
        "            \"titulo_ref\": bq.get(\"titulo\", np.nan)\n",
        "        })\n",
        "\n",
        "        bq_final = ensure_cols(bq_final, std)\n",
        "        bq_final[\"fuentes_presentes\"] = \"BQ\"\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# ESTANDARIZAR LOCALIZACIÓN\n",
        "# ================================================================\n",
        "CIUDADES_CO = {\n",
        "    \"bogota\":\"Bogotá\",\"bogotá\":\"Bogotá\",\"medellin\":\"Medellín\",\"medellín\":\"Medellín\",\n",
        "    \"cali\":\"Cali\",\"barranquilla\":\"Barranquilla\",\"cartagena\":\"Cartagena\",\"pereira\":\"Pereira\",\n",
        "    \"armenia\":\"Armenia\",\"manizales\":\"Manizales\",\"bucaramanga\":\"Bucaramanga\",\"cucuta\":\"Cúcuta\",\n",
        "    \"cúcuta\":\"Cúcuta\",\"ibague\":\"Ibagué\",\"ibagué\":\"Ibagué\",\"villavicencio\":\"Villavicencio\",\n",
        "    \"santa marta\":\"Santa Marta\",\"monteria\":\"Montería\",\"neiva\":\"Neiva\",\"tunja\":\"Tunja\",\n",
        "    \"yopal\":\"Yopal\",\"pasto\":\"Pasto\",\"popayan\":\"Popayán\",\"riohacha\":\"Riohacha\",\"quibdo\":\"Quibdó\",\n",
        "    \"leticia\":\"Leticia\",\"mocoa\":\"Mocoa\"\n",
        "}\n",
        "\n",
        "def _clean(s):\n",
        "    if s is None or (isinstance(s,float) and pd.isna(s)): return \"\"\n",
        "    return str(s).strip()\n",
        "\n",
        "def parse_region_to_pais_ciudad(val):\n",
        "    v = _clean(val)\n",
        "    if v == \"\" or v.upper() in {\"S/I\",\"S. I.\",\"NA\",\"N/A\"}:\n",
        "        return (\"Colombia\",\"\")\n",
        "    if \"colombia\" in v.lower():\n",
        "        partes = [p.strip() for p in re.split(r\",\", v) if p.strip()]\n",
        "        if len(partes) >= 2:\n",
        "            return (\"Colombia\", CIUDADES_CO.get(partes[0].lower(), partes[0]))\n",
        "    for k, pretty in CIUDADES_CO.items():\n",
        "        if re.search(rf\"\\b{k}\\b\", v.lower()):\n",
        "            return (\"Colombia\", pretty)\n",
        "    return (\"Colombia\", v[:80])\n",
        "\n",
        "def add_pais_ciudad(df):\n",
        "    if df.empty:\n",
        "        df[\"pais\"] = []\n",
        "        df[\"ciudad\"] = []\n",
        "        return df\n",
        "    p, c = zip(*df[\"region\"].apply(parse_region_to_pais_ciudad))\n",
        "    df[\"pais\"], df[\"ciudad\"] = p, c\n",
        "    return df\n",
        "\n",
        "sed_final = add_pais_ciudad(sed_final)\n",
        "web_final = add_pais_ciudad(web_final)\n",
        "bq_final  = add_pais_ciudad(bq_final)\n",
        "\n",
        "# ================================================================\n",
        "#  UNIR BASES\n",
        "# ================================================================\n",
        "cols_union = [\"fecha_evento\",\"pais\",\"ciudad\",\"organizaciones_sindicales\",\"tipo_movilizacion\",\n",
        "              \"duracion_dias\",\"razones_paro\",\"resumen\",\"titulo_ref\",\"urls\",\"fuentes_presentes\",\"region\"]\n",
        "\n",
        "for df in [sed_final, web_final, bq_final]:\n",
        "    if not df.empty:\n",
        "        for c in cols_union:\n",
        "            if c not in df.columns: df[c] = np.nan\n",
        "    else: # Ensure empty dfs have the required columns before concat\n",
        "         for c in cols_union:\n",
        "            if c not in df.columns:\n",
        "                df[c] = pd.Series([], dtype='object') # Use object dtype for mixed types\n",
        "\n",
        "\n",
        "base = pd.concat([sed_final[cols_union], web_final[cols_union], bq_final[cols_union]], ignore_index=True)\n",
        "base = base.sort_values([\"fecha_evento\",\"ciudad\"]).reset_index(drop=True)\n",
        "\n",
        "# ================================================================\n",
        "# UNIFICAR Y LIMPIAR ORGANIZACIONES SINDICALES\n",
        "# ================================================================\n",
        "def estandarizar_orgs(valor):\n",
        "    \"\"\"Limpia, normaliza y unifica nombres de sindicatos.\"\"\"\n",
        "    if pd.isna(valor):\n",
        "        return \"NO ESPECÍFICA\"\n",
        "\n",
        "    if isinstance(valor, (list, tuple, set)):\n",
        "        texto = \" | \".join(map(str, valor))\n",
        "    else:\n",
        "        texto = str(valor)\n",
        "\n",
        "    texto = re.sub(r\"[\\[\\]'\\\"{}]\", \"\", texto).upper()\n",
        "    partes = re.split(r\"[;|,/\\-]+\", texto)\n",
        "    partes = [p.strip() for p in partes if p.strip()]\n",
        "    if not partes:\n",
        "        return \"NO ESPECÍFICA\"\n",
        "\n",
        "    equivalencias = {\n",
        "        \"FEDERACION COLOMBIANA DE TRABAJADORES DE LA EDUCACION\": \"FECODE\",\n",
        "        \"FECODE\": \"FECODE\",\n",
        "        \"ADE\": \"ADE\",\n",
        "        \"ADIDA\": \"ADIDA\",\n",
        "        \"CUT\": \"CUT\",\n",
        "        \"CENTRAL UNITARIA DE TRABAJADORES\": \"CUT\",\n",
        "        \"CENTRAL UNITARIA DE TRABAJADORES DE COLOMBIA\": \"CUT\",\n",
        "        \"CONFEDERACION DE TRABAJADORES DE COLOMBIA\": \"CTC\",\n",
        "        \"CTC\": \"CTC\",\n",
        "        \"CGT\": \"CGT\",\n",
        "        \"CONFEDERACION GENERAL DEL TRABAJO\": \"CGT\",\n",
        "        \"SINTRANAL\": \"SINTRANAL\",\n",
        "        \"SINTRANAL EDU\": \"SINTRANAL\",\n",
        "        \"NO ESPECIFICA\": \"NO ESPECÍFICA\",\n",
        "        \"NO ESPECÍFICA\": \"NO ESPECÍFICA\"\n",
        "    }\n",
        "\n",
        "    normalizados = []\n",
        "    for p in partes:\n",
        "        p_clean = re.sub(r\"[^A-ZÁÉÍÓÚÜÑ ]\", \"\", p).strip()\n",
        "        for k, v in equivalencias.items():\n",
        "            if k in p_clean:\n",
        "                normalizados.append(v)\n",
        "                break\n",
        "        else:\n",
        "            normalizados.append(p_clean)\n",
        "\n",
        "    limpio = [x for x in sorted(set(normalizados)) if x]\n",
        "    if not limpio:\n",
        "        return \"NO ESPECÍFICA\"\n",
        "    return \" | \".join(limpio)\n",
        "\n",
        "if not base.empty:\n",
        "    base[\"organizaciones_sindicales\"] = base[\"organizaciones_sindicales\"].apply(estandarizar_orgs)\n",
        "\n",
        "# ================================================================\n",
        "# ESTADÍSTICAS DESCRIPTIVAS\n",
        "# ================================================================\n",
        "stats = {}\n",
        "if not base.empty:\n",
        "    stats = {\n",
        "        \"Total_paros\": len(base),\n",
        "        \"Promedio_duracion\": round(base[\"duracion_dias\"].mean(skipna=True), 2),\n",
        "        \"Max_duracion\": base[\"duracion_dias\"].max(),\n",
        "        \"Min_duracion\": base[\"duracion_dias\"].min(),\n",
        "        \"Tipos_movilizacion\": base[\"tipo_movilizacion\"].value_counts().to_dict(),\n",
        "        \"Ciudades_principales\": base[\"ciudad\"].value_counts().head(5).to_dict(),\n",
        "        \"Sindicatos_principales\": base[\"organizaciones_sindicales\"].value_counts().head(5).to_dict(),\n",
        "        \"Fuentes\": base[\"fuentes_presentes\"].value_counts().to_dict()\n",
        "    }\n",
        "    pd.DataFrame([stats]).to_csv(\"ESTADISTICAS_PAROS_DOCENTES_2015.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "else:\n",
        "    print(\"\\nNo data to generate statistics.\")\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# EXPORTAR BASE FINAL\n",
        "# ================================================================\n",
        "cols_export = [\"fecha_evento\",\"pais\",\"ciudad\",\"organizaciones_sindicales\",\"tipo_movilizacion\",\n",
        "               \"duracion_dias\",\"razones_paro\",\"resumen\",\"titulo_ref\",\"urls\",\"fuentes_presentes\"]\n",
        "\n",
        "if not base.empty:\n",
        "    base[\"fecha_evento\"] = pd.to_datetime(base[\"fecha_evento\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
        "    base[cols_export].to_csv(\"BASE_FINAL_PAROS_DOCENTES_2023_VALIDADA.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "    print(\"\\n✅ BASE_FINAL_PAROS_DOCENTES_2023_VALIDADA.csv generada correctamente\")\n",
        "    print(\"\\nResumen general:\")\n",
        "    for k, v in stats.items():\n",
        "        print(f\"- {k}: {v}\")\n",
        "else:\n",
        "    print(\"\\nNo data to export BASE_FINAL_PAROS_DOCENTES_2023_VALIDADA.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import json\n",
        "\n",
        "# ================================================================\n",
        "# CARGA DE BASE\n",
        "# ================================================================\n",
        "df = pd.read_csv(\"BASE_FINAL_PAROS_DOCENTES_2023_VALIDADA.csv\", encoding=\"utf-8-sig\")\n",
        "df.columns = df.columns.str.strip().str.lower()\n",
        "\n",
        "# ================================================================\n",
        "# FUNCIONES AUXILIARES\n",
        "# ================================================================\n",
        "def limpiar_texto(texto):\n",
        "    if pd.isna(texto):\n",
        "        return \"\"\n",
        "    return re.sub(r\"\\s+\", \" \", str(texto)).strip()\n",
        "\n",
        "def obtener_contenido_web(url):\n",
        "    if not isinstance(url, str) or url.strip().upper() in [\"\", \"S/I\", \"N/A\"]:\n",
        "        return \"\"\n",
        "    try:\n",
        "        r = requests.get(url, timeout=15)\n",
        "        if r.status_code != 200:\n",
        "            return \"\"\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        for tag in soup([\"script\", \"style\", \"noscript\", \"footer\", \"header\", \"aside\"]):\n",
        "            tag.extract()\n",
        "        texto = \" \".join(soup.stripped_strings)\n",
        "        return texto[:8000]\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "# ================================================================\n",
        "# ANALIZAR NOTICIA CON IA (GPT-4o MINI)\n",
        "# ================================================================\n",
        "def analizar_noticia_ia(fila):\n",
        "    url = fila.get(\"urls\", \"\")\n",
        "    titulo = fila.get(\"titulo_ref\", \"\")\n",
        "    texto = obtener_contenido_web(url)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Estandariza y completa la información de una noticia sobre paros docentes en Colombia.\n",
        "\n",
        "Entrada:\n",
        "Título: {titulo}\n",
        "Texto de la noticia: {texto[:1500]}\n",
        "\n",
        "Devuelve un JSON válido con las siguientes claves:\n",
        "{{\n",
        "  \"ubicacion_bogota\": true/false → si ocurre en Bogotá/Cundinamarca,\n",
        "  \"ubicacion_colombia\": true/false → si el paro abarca el territorio Nacional,\n",
        "  \"ciudad\": \"nombre de la ciudad (si se menciona o vacía si no)\",\n",
        "  \"tipo_movilizacion\": \"paro docente / plantón / marcha / toma / protesta / asamblea\",\n",
        "  \"duracion_dias\": número o null,\n",
        "  \"razones_paro\": \"resumen breve de motivos (salud, reformas, infraestructura, etc.)\",\n",
        "  \"resumen\": \"síntesis breve del hecho (máximo 3 líneas)\",\n",
        "  \"fecha_hecho\": \"fecha más probable del hecho en formato YYYY-MM-DD o vacío si no se menciona\"\n",
        "}}\n",
        "\n",
        "Si la información no está explícita, infiere con contexto general sobre paros docentes en Colombia.\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            temperature=0.2,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        )\n",
        "        salida = response.choices[0].message.content.strip()\n",
        "        try:\n",
        "            datos = json.loads(salida)\n",
        "        except:\n",
        "            match = re.search(r\"\\{.*\\}\", salida, re.DOTALL)\n",
        "            datos = json.loads(match.group(0)) if match else {}\n",
        "        return datos\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error IA: {e}\")\n",
        "        return {\n",
        "            \"ubicacion_bogota\": None,\n",
        "            \"ubicacion_colombia\": None,\n",
        "            \"ciudad\": \"\",\n",
        "            \"tipo_movilizacion\": \"\",\n",
        "            \"duracion_dias\": None,\n",
        "            \"razones_paro\": \"\",\n",
        "            \"resumen\": \"\",\n",
        "            \"fecha_hecho\": \"\"\n",
        "        }\n",
        "\n",
        "# ================================================================\n",
        "# DIVIDIR ENTRE ANALIZABLES Y NO ANALIZABLES\n",
        "# ================================================================\n",
        "df[\"fuentes_presentes\"] = df[\"fuentes_presentes\"].astype(str).str.upper().str.strip()\n",
        "mask_ia = df[\"fuentes_presentes\"].isin([\"SED\", \"BQ\"]) & df[\"urls\"].notna() & ~df[\"urls\"].str.upper().isin([\"S/I\", \"N/A\", \"\"])\n",
        "df_analizar = df[mask_ia].copy()\n",
        "df_conservar = df[~mask_ia].copy()\n",
        "\n",
        "print(f\"Noticias a analizar con IA: {len(df_analizar)}\")\n",
        "print(f\"Noticias que se conservarán sin análisis IA: {len(df_conservar)}\")\n",
        "\n",
        "# ================================================================\n",
        "# PROCESAR CON IA Y RELLENAR SI FALTA INFO\n",
        "# ================================================================\n",
        "resultados_ia = []\n",
        "\n",
        "for i, fila in enumerate(df_analizar.to_dict(\"records\")):\n",
        "    print(f\"[{i+1}/{len(df_analizar)}] Analizando IA: {fila.get('urls', '')[:80]}\")\n",
        "    try:\n",
        "        analisis = analizar_noticia_ia(fila)\n",
        "\n",
        "        # Si la IA no devuelve fecha_hecho, usar 'fecha_evento'\n",
        "        if not analisis.get(\"fecha_hecho\"):\n",
        "            analisis[\"fecha_hecho\"] = fila.get(\"fecha_evento\", \"\")\n",
        "\n",
        "        # Si la IA no devuelve duracion_dias, usar 'duracion_dias' de la base\n",
        "        if not analisis.get(\"duracion_dias\"):\n",
        "            try:\n",
        "                dias = int(fila.get(\"duracion_dias\", 0))\n",
        "                analisis[\"duracion_dias\"] = dias if dias > 0 else None\n",
        "            except:\n",
        "                analisis[\"duracion_dias\"] = None\n",
        "\n",
        "        fila.update(analisis)\n",
        "        resultados_ia.append(fila)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error en fila {i+1}: {e}\")\n",
        "        resultados_ia.append(fila)\n",
        "\n",
        "    time.sleep(2)\n",
        "\n",
        "df_ia = pd.DataFrame(resultados_ia)\n",
        "\n",
        "# ================================================================\n",
        "# UNIR TODO Y LIMPIAR COLUMNAS AUXILIARES\n",
        "# ================================================================\n",
        "df_final = pd.concat([df_conservar, df_ia], ignore_index=True)\n",
        "\n",
        "columnas_a_eliminar = [\"ubicacion_bogota\", \"ubicacion_colombia\", \"ciudad\", \"fecha_hecho\"]\n",
        "df_final = df_final.drop(columns=[col for col in columnas_a_eliminar if col in df_final.columns])\n",
        "\n",
        "# === Guardar resultado si se desea ===\n",
        "# df_final.to_csv(\"Paros_Docentes_Analizados.csv\", index=False, encoding=\"utf-8-sig\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhAT6Iy5mSMd",
        "outputId": "d5307798-ff3b-4161-c598-9e5fc7025096"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noticias a analizar con IA: 5\n",
            "Noticias que se conservarán sin análisis IA: 6\n",
            "[1/5] Analizando IA: https://groups.google.com/g/mesa-distrital-orientadores-escolares-/c/SPAiGj6kOUc\n",
            "[2/5] Analizando IA: https://www.pares.com.co/post/impactos-que-tuvo-el-paro-agrario?utm_source\n",
            "[3/5] Analizando IA: https://ocecolombia.co/movilizacion-distrital-de-la-ade-el-7-de-febrero/\n",
            "[4/5] Analizando IA: https://www.bbc.com/mundo/ultimas_noticias/2013/09/130910_ultnot_educacion_colom\n",
            "[5/5] Analizando IA: https://elpais.com/economia/2013/09/11/agencias/1378931240_545234.html?utm_sourc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmu6oQN5CCb_",
        "outputId": "9b004c3f-fd0b-4ba4-9327-5fc573b994ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proceso completado correctamente.\n",
            "Archivo generado: BASE_PAROS_DOCENTES_2023_IA.csv\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "# ESTANDARIZAR ORGANIZACIONES SINDICALES\n",
        "# ================================================================\n",
        "def estandarizar_orgs(valor):\n",
        "    \"\"\"Estandariza nombres de sindicatos en formato único y consistente.\"\"\"\n",
        "    # Manejar valores vacíos, listas o arrays\n",
        "    if valor is None:\n",
        "        return \"\"\n",
        "    if isinstance(valor, (list, set, tuple)):\n",
        "        texto = \" \".join(map(str, valor))\n",
        "    elif isinstance(valor, (pd.Series, np.ndarray)):\n",
        "        texto = \" \".join(map(str, valor.tolist()))\n",
        "    else:\n",
        "        texto = str(valor)\n",
        "\n",
        "    texto = texto.strip()\n",
        "    if texto == \"\" or texto.upper() in [\"NAN\", \"NONE\"]:\n",
        "        return \"NO ESPECÍFICA\"\n",
        "\n",
        "    texto = re.sub(r\"[\\[\\]'\\\"{}]\", \"\", texto)\n",
        "    partes = re.split(r\"[;|,/\\-]+\", texto)\n",
        "    partes = [p.strip().upper() for p in partes if p.strip()]\n",
        "\n",
        "    equivalencias = {\n",
        "        \"ADE\": [\"ADE\", \"ASOCIACION DISTRITAL DE EDUCADORES\", \"ASOCIACIÓN DISTRITAL DE EDUCADORES\"],\n",
        "        \"FECODE\": [\"FECODE\", \"FEDERACION COLOMBIANA DE TRABAJADORES DE LA EDUCACION\", \"FEDERACIÓN COLOMBIANA DE TRABAJADORES DE LA EDUCACIÓN\"],\n",
        "        \"CUT\": [\"CUT\", \"CENTRAL UNITARIA DE TRABAJADORES\"],\n",
        "        \"CTC\": [\"CTC\", \"CONFEDERACION DE TRABAJADORES DE COLOMBIA\", \"CONFEDERACIÓN DE TRABAJADORES DE COLOMBIA\"],\n",
        "        \"SINTRANAL\": [\"SINTRANAL\", \"SINDICATO NACIONAL DE TRABAJADORES DE LA EDUCACION\", \"SINDICATO NACIONAL DE TRABAJADORES DE LA EDUCACIÓN\"],\n",
        "        \"ADIDA\": [\"ADIDA\", \"ASOCIACION DE INSTITUTO DE ANTIOQUIA\", \"ASOCIACIÓN DE INSTITUTO DE ANTIOQUIA\"]\n",
        "    }\n",
        "\n",
        "    normalizados = set()\n",
        "    for p in partes:\n",
        "        for clave, variantes in equivalencias.items():\n",
        "            if any(v in p for v in variantes):\n",
        "                normalizados.add(clave)\n",
        "                break\n",
        "        else:\n",
        "            if p and p not in [\"NO ESPECIFICA\", \"NO ESPECÍFICA\", \"NO APLICA\"]:\n",
        "                normalizados.add(p)\n",
        "\n",
        "    return \" | \".join(sorted(normalizados)) if normalizados else \"NO ESPECÍFICA\"\n",
        "\n",
        "\n",
        "df_final [\"organizaciones_sindicales\"] = df_final[\"organizaciones_sindicales\"].apply(estandarizar_orgs)\n",
        "\n",
        "# ================================================================\n",
        "# GUARDAR RESULTADO FINAL\n",
        "# ================================================================\n",
        "df_final.to_csv(\"BASE_PAROS_DOCENTES_2023_IA.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"Proceso completado correctamente.\")\n",
        "print(\"Archivo generado: BASE_PAROS_DOCENTES_2023_IA.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# UNIFICAR PAROS DOCENTES 2015  (duración desde inicio/fin si falta)\n",
        "# ================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import timedelta\n",
        "\n",
        "# -----------------------------\n",
        "# 1. CARGAR BASE\n",
        "# -----------------------------\n",
        "# Si tu archivo fuente ya es de 2015, usa ese nombre:\n",
        "# df = pd.read_csv(\"BASE_PAROS_DOCENTES_2015_IA.csv\", encoding=\"utf-8-sig\")\n",
        "# Si aún usas el de 2023 pero filtrado a 2015, déjalo así:\n",
        "df = pd.read_csv(\"BASE_PAROS_DOCENTES_2023_IA.csv\", encoding=\"utf-8-sig\")\n",
        "df.columns = df.columns.str.strip().str.lower()\n",
        "\n",
        "if \"fecha_evento\" in df.columns:\n",
        "    df[\"fecha_evento\"] = pd.to_datetime(df[\"fecha_evento\"], errors=\"coerce\")\n",
        "\n",
        "df = df.dropna(subset=[\"fecha_evento\"]).sort_values(\"fecha_evento\").reset_index(drop=True)\n",
        "\n",
        "# -----------------------------\n",
        "# 2. FUNCIONES AUXILIARES\n",
        "# -----------------------------\n",
        "def combinar_textos(series):\n",
        "    \"\"\"Combina textos o listas de textos sin duplicar información.\"\"\"\n",
        "    textos = []\n",
        "    for val in series.dropna():\n",
        "        if isinstance(val, str):\n",
        "            partes = re.split(r\"[|;]+\", val)\n",
        "            textos.extend([p.strip() for p in partes if p.strip()])\n",
        "        elif isinstance(val, (list, set, tuple)):\n",
        "            textos.extend([str(p).strip() for p in val if str(p).strip()])\n",
        "    return \"; \".join(sorted(set(textos)))\n",
        "\n",
        "def combinar_numeros(series):\n",
        "    \"\"\"Promedio de valores numéricos.\"\"\"\n",
        "    vals = pd.to_numeric(series, errors=\"coerce\")\n",
        "    return round(vals.mean(skipna=True), 1) if vals.notna().any() else np.nan\n",
        "\n",
        "def combinar_fuentes(series):\n",
        "    \"\"\"Une todas las fuentes sin repetir.\"\"\"\n",
        "    return \"; \".join(sorted(set(\n",
        "        re.split(r\"[;|,]+\", \";\".join(series.dropna().astype(str)))\n",
        "    )))\n",
        "\n",
        "def combinar_urls(series):\n",
        "    \"\"\"Combina todas las URLs únicas, creando columnas url_1, url_2, ...\"\"\"\n",
        "    urls = []\n",
        "    for val in series.dropna():\n",
        "        if isinstance(val, str):\n",
        "            for u in re.split(r\"[|,;]+\", val):\n",
        "                if u.strip() and u.strip().startswith(\"http\"):\n",
        "                    urls.append(u.strip())\n",
        "    return sorted(set(urls))\n",
        "\n",
        "# -----------------------------\n",
        "# 3. AGRUPAR POR FECHA CON TOLERANCIA DE ±1 DÍA (mismo evento)\n",
        "# -----------------------------\n",
        "df = df.sort_values(\"fecha_evento\").reset_index(drop=True)\n",
        "\n",
        "grupos = []\n",
        "grupo_actual = 0\n",
        "fechas = df[\"fecha_evento\"].tolist()\n",
        "\n",
        "for i in range(len(fechas)):\n",
        "    if i == 0:\n",
        "        grupos.append(grupo_actual)\n",
        "        continue\n",
        "    diff = (fechas[i] - fechas[i - 1]).days\n",
        "    if diff <= 3:\n",
        "        grupos.append(grupo_actual)\n",
        "    else:\n",
        "        grupo_actual += 1\n",
        "        grupos.append(grupo_actual)\n",
        "\n",
        "df[\"grupo_paro\"] = grupos\n",
        "\n",
        "# -----------------------------\n",
        "# 4. FUSIONAR INFORMACIÓN POR GRUPO\n",
        "# -----------------------------\n",
        "agrupado = []\n",
        "for gid, sub in df.groupby(\"grupo_paro\"):\n",
        "    fila = {}\n",
        "    fila[\"fecha_inicio\"] = sub[\"fecha_evento\"].min()\n",
        "    fila[\"fecha_fin\"]   = sub[\"fecha_evento\"].max()\n",
        "    fila[\"ciudad\"] = combinar_textos(sub.get(\"ciudad\", pd.Series([], dtype=object)))\n",
        "    fila[\"organizaciones_sindicales\"] = combinar_textos(sub.get(\"organizaciones_sindicales\", pd.Series([], dtype=object)))\n",
        "    fila[\"tipo_movilizacion\"] = combinar_textos(sub.get(\"tipo_movilizacion\", pd.Series([], dtype=object)))\n",
        "    # Promedio si venía reportado; si no, luego lo recalculamos desde fechas\n",
        "    fila[\"duracion_dias\"] = combinar_numeros(sub.get(\"duracion_dias\", pd.Series([], dtype=object)))\n",
        "    fila[\"razones_paro\"] = combinar_textos(sub.get(\"razones_paro\", pd.Series([], dtype=object)))\n",
        "    fila[\"resumen\"] = combinar_textos(sub.get(\"resumen\", pd.Series([], dtype=object)))\n",
        "    fila[\"fuentes_presentes\"] = combinar_fuentes(sub.get(\"fuentes_presentes\", pd.Series([], dtype=object)))\n",
        "\n",
        "    urls_unicas = combinar_urls(sub.get(\"urls\", pd.Series([], dtype=object)))\n",
        "    for i, url in enumerate(urls_unicas, start=1):\n",
        "        fila[f\"url_{i}\"] = url\n",
        "\n",
        "    agrupado.append(fila)\n",
        "\n",
        "base_unificada = pd.DataFrame(agrupado).sort_values(\"fecha_inicio\").reset_index(drop=True)\n",
        "\n",
        "# -----------------------------\n",
        "# 4.1 CALCULAR DURACIÓN DESDE INICIO/FIN **solo si duracion_dias está vacía**\n",
        "# -----------------------------\n",
        "# Parse seguro de fechas (en caso de venir como string)\n",
        "base_unificada[\"fecha_inicio\"] = pd.to_datetime(base_unificada[\"fecha_inicio\"], errors=\"coerce\")\n",
        "base_unificada[\"fecha_fin\"]    = pd.to_datetime(base_unificada[\"fecha_fin\"], errors=\"coerce\")\n",
        "\n",
        "# Máscara: filas donde duracion_dias está NaN y SÍ tenemos ambas fechas\n",
        "mask_calc = base_unificada[\"duracion_dias\"].isna() & base_unificada[\"fecha_inicio\"].notna() & base_unificada[\"fecha_fin\"].notna()\n",
        "\n",
        "# Diferencia inclusiva: si inicio == fin -> 1 día; si no, (fin - inicio).days + 1\n",
        "duracion_calc = (base_unificada.loc[mask_calc, \"fecha_fin\"] - base_unificada.loc[mask_calc, \"fecha_inicio\"]).dt.days + 1\n",
        "# Evitar valores no positivos por si hay desorden en fechas (forzar mínimo 1)\n",
        "duracion_calc = duracion_calc.clip(lower=1)\n",
        "\n",
        "# Asignar solo a las filas que cumplen la máscara\n",
        "base_unificada.loc[mask_calc, \"duracion_dias\"] = duracion_calc.astype(\"Int64\")\n",
        "\n",
        "# -----------------------------\n",
        "# 5. EXPORTAR RESULTADO\n",
        "# -----------------------------\n",
        "# Salida en formato ISO\n",
        "base_unificada[\"fecha_inicio\"] = base_unificada[\"fecha_inicio\"].dt.strftime(\"%Y-%m-%d\")\n",
        "base_unificada[\"fecha_fin\"]    = base_unificada[\"fecha_fin\"].dt.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "base_unificada.to_csv(\"BASE_PAROS_UNIFICADOS_2015.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"Base unificada generada: BASE_PAROS_UNIFICADOS_2013.csv\")\n",
        "print(\"Paros únicos encontrados:\", len(base_unificada))\n",
        "print(\"Número máximo de columnas URL:\", base_unificada.filter(like='url_').shape[1])\n",
        "print(\"Filas con duración calculada por fechas:\", mask_calc.sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liAc2raGuuMD",
        "outputId": "9e2bbd95-3ade-46f3-ba78-c58610281928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base unificada generada: BASE_PAROS_UNIFICADOS_2015.csv\n",
            "Paros únicos encontrados: 9\n",
            "Número máximo de columnas URL: 2\n",
            "Filas con duración calculada por fechas: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import timedelta\n",
        "\n",
        "# -----------------------------\n",
        "# 1. CARGAR BASE\n",
        "# -----------------------------\n",
        "df = pd.read_csv(\"BASE_FINAL_PAROS_DOCENTES_2023_VALIDADA.csv\", encoding=\"utf-8-sig\")\n",
        "df.columns = df.columns.str.strip().str.lower()\n",
        "\n",
        "# Normalizar fechas\n",
        "df[\"fecha_evento\"] = pd.to_datetime(df[\"fecha_evento\"], errors=\"coerce\")\n",
        "df = df.dropna(subset=[\"fecha_evento\"]).sort_values(\"fecha_evento\").reset_index(drop=True)\n",
        "\n",
        "# -----------------------------\n",
        "# 2. FUNCIONES AUXILIARES\n",
        "# -----------------------------\n",
        "def combinar_textos(series):\n",
        "    textos = []\n",
        "    for val in series.dropna():\n",
        "        if isinstance(val, str):\n",
        "            partes = re.split(r\"[|;]+\", val)\n",
        "            textos.extend([p.strip() for p in partes if p.strip()])\n",
        "        elif isinstance(val, (list, set, tuple)):\n",
        "            textos.extend([str(p).strip() for p in val if str(p).strip()])\n",
        "    return \"; \".join(sorted(set(textos)))\n",
        "\n",
        "def combinar_numeros(series):\n",
        "    vals = pd.to_numeric(series, errors=\"coerce\")\n",
        "    return round(vals.mean(skipna=True), 1) if vals.notna().any() else np.nan\n",
        "\n",
        "def combinar_fuentes(series):\n",
        "    return \"; \".join(sorted(set(re.split(r\"[;|,]+\", \";\".join(series.dropna().astype(str))))))\n",
        "\n",
        "def combinar_urls(series):\n",
        "    urls = []\n",
        "    for val in series.dropna():\n",
        "        if isinstance(val, str):\n",
        "            for u in re.split(r\"[|,;]+\", val):\n",
        "                if u.strip() and u.strip().startswith(\"http\"):\n",
        "                    urls.append(u.strip())\n",
        "    return sorted(set(urls))\n",
        "\n",
        "# -----------------------------\n",
        "# 3. AGRUPAR POR FECHA CON TOLERANCIA DE ±3 DÍAS\n",
        "# -----------------------------\n",
        "df = df.sort_values(\"fecha_evento\").reset_index(drop=True)\n",
        "grupos = []\n",
        "grupo_actual = 0\n",
        "fechas = df[\"fecha_evento\"].tolist()\n",
        "\n",
        "for i in range(len(fechas)):\n",
        "    if i == 0:\n",
        "        grupos.append(grupo_actual)\n",
        "        continue\n",
        "    diff = (fechas[i] - fechas[i - 1]).days\n",
        "    if diff <= 3:\n",
        "        grupos.append(grupo_actual)\n",
        "    else:\n",
        "        grupo_actual += 1\n",
        "        grupos.append(grupo_actual)\n",
        "\n",
        "df[\"grupo_paro\"] = grupos\n",
        "\n",
        "# -----------------------------\n",
        "# 4. FUSIONAR INFORMACIÓN POR GRUPO Y CALCULAR FECHA FIN\n",
        "# -----------------------------\n",
        "agrupado = []\n",
        "for gid, sub in df.groupby(\"grupo_paro\"):\n",
        "    fila = {}\n",
        "\n",
        "    # Fecha de inicio: mínima\n",
        "    fecha_inicio = sub[\"fecha_evento\"].min()\n",
        "\n",
        "    # Duración: promedio o 1 como mínimo\n",
        "    duracion_dias = combinar_numeros(sub.get(\"duracion_dias\", pd.Series([], dtype=object)))\n",
        "    if pd.isna(duracion_dias) or duracion_dias < 1:\n",
        "        duracion_dias = 1\n",
        "\n",
        "    # Fecha de fin = fecha_inicio + duración - 1\n",
        "    fecha_fin = fecha_inicio + timedelta(days=int(duracion_dias) - 1)\n",
        "\n",
        "    fila[\"fecha_inicio\"] = fecha_inicio\n",
        "    fila[\"fecha_fin\"] = fecha_fin\n",
        "    fila[\"duracion_dias\"] = int(duracion_dias)\n",
        "\n",
        "    fila[\"ciudad\"] = combinar_textos(sub.get(\"ciudad\", pd.Series([], dtype=object)))\n",
        "    fila[\"organizaciones_sindicales\"] = combinar_textos(sub.get(\"organizaciones_sindicales\", pd.Series([], dtype=object)))\n",
        "    fila[\"tipo_movilizacion\"] = combinar_textos(sub.get(\"tipo_movilizacion\", pd.Series([], dtype=object)))\n",
        "    fila[\"razones_paro\"] = combinar_textos(sub.get(\"razones_paro\", pd.Series([], dtype=object)))\n",
        "    fila[\"resumen\"] = combinar_textos(sub.get(\"resumen\", pd.Series([], dtype=object)))\n",
        "    fila[\"fuentes_presentes\"] = combinar_fuentes(sub.get(\"fuentes_presentes\", pd.Series([], dtype=object)))\n",
        "\n",
        "    urls_unicas = combinar_urls(sub.get(\"urls\", pd.Series([], dtype=object)))\n",
        "    for i, url in enumerate(urls_unicas, start=1):\n",
        "        fila[f\"url_{i}\"] = url\n",
        "\n",
        "    agrupado.append(fila)\n",
        "\n",
        "# -----------------------------\n",
        "# 5. RESULTADO FINAL\n",
        "# -----------------------------\n",
        "base_unificada = pd.DataFrame(agrupado).sort_values(\"fecha_inicio\").reset_index(drop=True)\n",
        "\n",
        "# -----------------------------\n",
        "# 4.1 CALCULAR DURACIÓN DESDE INICIO/FIN **solo si duracion_dias está vacía**\n",
        "# -----------------------------\n",
        "# Parse seguro de fechas (en caso de venir como string)\n",
        "base_unificada[\"fecha_inicio\"] = pd.to_datetime(base_unificada[\"fecha_inicio\"], errors=\"coerce\")\n",
        "base_unificada[\"fecha_fin\"]    = pd.to_datetime(base_unificada[\"fecha_fin\"], errors=\"coerce\")\n",
        "\n",
        "# Máscara: filas donde duracion_dias está NaN y SÍ tenemos ambas fechas\n",
        "mask_calc = base_unificada[\"duracion_dias\"].isna() & base_unificada[\"fecha_inicio\"].notna() & base_unificada[\"fecha_fin\"].notna()\n",
        "\n",
        "# Diferencia inclusiva: si inicio == fin -> 1 día; si no, (fin - inicio).days + 1\n",
        "duracion_calc = (base_unificada.loc[mask_calc, \"fecha_fin\"] - base_unificada.loc[mask_calc, \"fecha_inicio\"]).dt.days + 1\n",
        "# Evitar valores no positivos por si hay desorden en fechas (forzar mínimo 1)\n",
        "duracion_calc = duracion_calc.clip(lower=1)\n",
        "\n",
        "# Asignar solo a las filas que cumplen la máscara\n",
        "base_unificada.loc[mask_calc, \"duracion_dias\"] = duracion_calc.astype(\"Int64\")\n",
        "\n",
        "# -----------------------------\n",
        "# 5. EXPORTAR RESULTADO\n",
        "# -----------------------------\n",
        "# Salida en formato ISO\n",
        "base_unificada[\"fecha_inicio\"] = base_unificada[\"fecha_inicio\"].dt.strftime(\"%Y-%m-%d\")\n",
        "base_unificada[\"fecha_fin\"]    = base_unificada[\"fecha_fin\"].dt.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "base_unificada.to_csv(\"BASE_PAROS_UNIFICADOS_2013.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"Base unificada generada: BASE_PAROS_UNIFICADOS_2013.csv\")\n",
        "print(\"Paros únicos encontrados:\", len(base_unificada))\n",
        "print(\"Número máximo de columnas URL:\", base_unificada.filter(like='url_').shape[1])\n",
        "print(\"Filas con duración calculada por fechas:\", mask_calc.sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HD2pIuOyoOMK",
        "outputId": "33d1c8c9-15b7-4af2-aed7-91500295817e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base unificada generada: BASE_PAROS_UNIFICADOS_2013.csv\n",
            "Paros únicos encontrados: 9\n",
            "Número máximo de columnas URL: 2\n",
            "Filas con duración calculada por fechas: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C30UpnA82cPc",
        "outputId": "393b0728-555b-4f3f-804a-6c142bd7f2c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base unificada generada: BASE_PAROS_UNIFICADOS_2015.csv\n",
            "Paros únicos encontrados: 8\n",
            "Número máximo de columnas URL: 3\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "# UNIFICAR PAROS DOCENTES  2015\n",
        "# ================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import timedelta\n",
        "\n",
        "# -----------------------------\n",
        "# 1. CARGAR BASE\n",
        "# -----------------------------\n",
        "df = pd.read_csv(\"BASE_PAROS_DOCENTES_2023_IA.csv\", encoding=\"utf-8-sig\")\n",
        "df.columns = df.columns.str.strip().str.lower()\n",
        "\n",
        "if \"fecha_evento\" in df.columns:\n",
        "    df[\"fecha_evento\"] = pd.to_datetime(df[\"fecha_evento\"], errors=\"coerce\")\n",
        "\n",
        "df = df.dropna(subset=[\"fecha_evento\"]).sort_values(\"fecha_evento\").reset_index(drop=True)\n",
        "\n",
        "# -----------------------------\n",
        "# 2. FUNCIONES AUXILIARES\n",
        "# -----------------------------\n",
        "def combinar_textos(series):\n",
        "    \"\"\"Combina textos o listas de textos sin duplicar información.\"\"\"\n",
        "    textos = []\n",
        "    for val in series.dropna():\n",
        "        if isinstance(val, str):\n",
        "            partes = re.split(r\"[|;]+\", val)\n",
        "            textos.extend([p.strip() for p in partes if p.strip()])\n",
        "        elif isinstance(val, (list, set, tuple)):\n",
        "            textos.extend([str(p).strip() for p in val if str(p).strip()])\n",
        "    return \"; \".join(sorted(set(textos)))\n",
        "\n",
        "def combinar_numeros(series):\n",
        "    \"\"\"Promedio de valores numéricos.\"\"\"\n",
        "    vals = pd.to_numeric(series, errors=\"coerce\")\n",
        "    return round(vals.mean(skipna=True), 1) if vals.notna().any() else np.nan\n",
        "\n",
        "def combinar_fuentes(series):\n",
        "    \"\"\"Une todas las fuentes sin repetir.\"\"\"\n",
        "    return \"; \".join(sorted(set(\n",
        "        re.split(r\"[;|,]+\", \";\".join(series.dropna().astype(str)))\n",
        "    )))\n",
        "\n",
        "def combinar_urls(series):\n",
        "    \"\"\"Combina todas las URLs únicas, creando columnas url_1, url_2, ...\"\"\"\n",
        "    urls = []\n",
        "    for val in series.dropna():\n",
        "        if isinstance(val, str):\n",
        "            for u in re.split(r\"[|,;]+\", val):\n",
        "                if u.strip() and u.strip().startswith(\"http\"):\n",
        "                    urls.append(u.strip())\n",
        "    return sorted(set(urls))\n",
        "\n",
        "# -----------------------------\n",
        "# 3. AGRUPAR POR FECHA CON TOLERANCIA DE ±2 DÍAS\n",
        "# -----------------------------\n",
        "# Ordenar por fecha\n",
        "df = df.sort_values(\"fecha_evento\").reset_index(drop=True)\n",
        "\n",
        "# Crear grupos de eventos cercanos (ventana de 2 días)\n",
        "grupos = []\n",
        "grupo_actual = 0\n",
        "fechas = df[\"fecha_evento\"].tolist()\n",
        "\n",
        "for i in range(len(fechas)):\n",
        "    if i == 0:\n",
        "        grupos.append(grupo_actual)\n",
        "        continue\n",
        "    diff = (fechas[i] - fechas[i - 1]).days\n",
        "    if diff <= 1:\n",
        "        grupos.append(grupo_actual)\n",
        "    else:\n",
        "        grupo_actual += 1\n",
        "        grupos.append(grupo_actual)\n",
        "\n",
        "df[\"grupo_paro\"] = grupos\n",
        "\n",
        "# -----------------------------\n",
        "# 4. FUSIONAR INFORMACIÓN POR GRUPO\n",
        "# -----------------------------\n",
        "agrupado = []\n",
        "for gid, sub in df.groupby(\"grupo_paro\"):\n",
        "    fila = {}\n",
        "    fila[\"fecha_inicio\"] = sub[\"fecha_evento\"].min()\n",
        "    fila[\"fecha_fin\"] = sub[\"fecha_evento\"].max()\n",
        "    fila[\"ciudad\"] = combinar_textos(sub.get(\"ciudad\", pd.Series([])))\n",
        "    fila[\"organizaciones_sindicales\"] = combinar_textos(sub.get(\"organizaciones_sindicales\", pd.Series([])))\n",
        "    fila[\"tipo_movilizacion\"] = combinar_textos(sub.get(\"tipo_movilizacion\", pd.Series([])))\n",
        "    fila[\"duracion_dias\"] = combinar_numeros(sub.get(\"duracion_dias\", pd.Series([])))\n",
        "    fila[\"razones_paro\"] = combinar_textos(sub.get(\"razones_paro\", pd.Series([])))\n",
        "    fila[\"resumen\"] = combinar_textos(sub.get(\"resumen\", pd.Series([])))\n",
        "    fila[\"fuentes_presentes\"] = combinar_fuentes(sub.get(\"fuentes_presentes\", pd.Series([])))\n",
        "\n",
        "    urls_unicas = combinar_urls(sub.get(\"urls\", pd.Series([])))\n",
        "    for i, url in enumerate(urls_unicas, start=1):\n",
        "        fila[f\"url_{i}\"] = url\n",
        "\n",
        "    agrupado.append(fila)\n",
        "\n",
        "base_unificada = pd.DataFrame(agrupado).sort_values(\"fecha_inicio\").reset_index(drop=True)\n",
        "\n",
        "# -----------------------------\n",
        "# 5. EXPORTAR RESULTADO\n",
        "# -----------------------------\n",
        "base_unificada[\"fecha_inicio\"] = pd.to_datetime(base_unificada[\"fecha_inicio\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
        "base_unificada[\"fecha_fin\"] = pd.to_datetime(base_unificada[\"fecha_fin\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "base_unificada.to_csv(\"BASE_PAROS_UNIFICADOS_2015.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"Base unificada generada: BASE_PAROS_UNIFICADOS_2015.csv\")\n",
        "print(\"Paros únicos encontrados:\", len(base_unificada))\n",
        "print(\"Número máximo de columnas URL:\", base_unificada.filter(like='url_').shape[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLPEnynp3aN2",
        "outputId": "0f19e05c-03e5-4e40-ac7e-7a5d5ede7b14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generando estadísticas descriptivas de paros unificados...\n",
            "\n",
            "Resumen rápido:\n",
            "Total de paros únicos: 8\n",
            "Meses con paros: 7\n",
            "Promedio de duración: 12.33 días\n",
            "Rango de fechas: 2015-01-01 → 2015-07-07\n",
            "\n",
            "Archivo generado: Estadisticas_Paros_Unificados_2015.xlsx\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "# ESTADÍSTICAS BASE PAROS UNIFICADOS\n",
        "# ================================================================\n",
        "\n",
        "base = pd.read_csv(\"BASE_PAROS_UNIFICADOS_2015.csv\", encoding=\"utf-8-sig\")\n",
        "base.columns = base.columns.str.strip().str.lower()\n",
        "\n",
        "# --- Fechas ---\n",
        "if \"fecha_inicio\" in base.columns:\n",
        "    base[\"fecha_inicio\"] = pd.to_datetime(base[\"fecha_inicio\"], errors=\"coerce\")\n",
        "if \"fecha_fin\" in base.columns:\n",
        "    base[\"fecha_fin\"] = pd.to_datetime(base[\"fecha_fin\"], errors=\"coerce\")\n",
        "\n",
        "# --- Duración calculada si falta ---\n",
        "if \"duracion_dias\" not in base.columns or base[\"duracion_dias\"].isna().all():\n",
        "    base[\"duracion_dias\"] = (base[\"fecha_fin\"] - base[\"fecha_inicio\"]).dt.days + 1\n",
        "\n",
        "# Asegurar valores positivos\n",
        "base[\"duracion_dias\"] = base[\"duracion_dias\"].clip(lower=1)\n",
        "\n",
        "# ================================================================\n",
        "# 2. LIMPIEZA Y PREPARACIÓN\n",
        "# ================================================================\n",
        "def limpiar_texto(x):\n",
        "    if pd.isna(x):\n",
        "        return \"\"\n",
        "    x = str(x).strip().lower()\n",
        "    x = re.sub(r\"[\\[\\]{}'\\\"]\", \"\", x)\n",
        "    return re.sub(r\"\\s+\", \" \", x).strip()\n",
        "\n",
        "base[\"tipo_movilizacion\"] = base[\"tipo_movilizacion\"].apply(limpiar_texto)\n",
        "base[\"organizaciones_sindicales\"] = base[\"organizaciones_sindicales\"].apply(limpiar_texto)\n",
        "base[\"fuentes_presentes\"] = base[\"fuentes_presentes\"].apply(limpiar_texto)\n",
        "\n",
        "# Mes de referencia según fecha_inicio\n",
        "base[\"mes\"] = base[\"fecha_inicio\"].dt.to_period(\"M\")\n",
        "\n",
        "# ================================================================\n",
        "# 3. ESTADÍSTICAS DESCRIPTIVAS\n",
        "# ================================================================\n",
        "print(\"\\nGenerando estadísticas descriptivas de paros unificados...\")\n",
        "\n",
        "# ---- 3.1 Total de paros por mes\n",
        "paros_mes = base[\"mes\"].value_counts().sort_index().reset_index()\n",
        "paros_mes.columns = [\"Mes\", \"Total_Paros\"]\n",
        "\n",
        "# ---- 3.2 Tipos de movilización\n",
        "tipos = (\n",
        "    base[\"tipo_movilizacion\"]\n",
        "    .str.split(r\"[;|,]+\")\n",
        "    .explode()\n",
        "    .str.strip()\n",
        "    .replace(\"\", np.nan)\n",
        "    .dropna()\n",
        "    .value_counts()\n",
        "    .reset_index()\n",
        ")\n",
        "tipos.columns = [\"Tipo de Movilización\", \"Total\"]\n",
        "\n",
        "# ---- 3.3 Duración promedio, mediana y rango\n",
        "base[\"duracion_dias\"] = pd.to_numeric(base[\"duracion_dias\"], errors=\"coerce\")\n",
        "duracion_stats = {\n",
        "    \"Promedio (días)\": round(base[\"duracion_dias\"].mean(skipna=True), 2),\n",
        "    \"Mediana (días)\": base[\"duracion_dias\"].median(skipna=True),\n",
        "    \"Máximo (días)\": base[\"duracion_dias\"].max(),\n",
        "    \"Mínimo (días)\": base[\"duracion_dias\"].min(),\n",
        "    \"Desviación estándar\": round(base[\"duracion_dias\"].std(skipna=True), 2)\n",
        "}\n",
        "\n",
        "# ---- 3.4 Organizaciones sindicales\n",
        "def limpiar_orgs(x):\n",
        "    if pd.isna(x):\n",
        "        return []\n",
        "    if isinstance(x, str):\n",
        "        parts = re.split(r\"[,;|]+\", x)\n",
        "        return [p.strip().upper() for p in parts if p.strip()]\n",
        "    return []\n",
        "\n",
        "orgs = (\n",
        "    base[\"organizaciones_sindicales\"]\n",
        "    .apply(limpiar_orgs)\n",
        "    .explode()\n",
        "    .value_counts()\n",
        "    .head(15)\n",
        "    .reset_index()\n",
        ")\n",
        "orgs.columns = [\"Organización Sindical\", \"Frecuencia\"]\n",
        "\n",
        "# ---- 3.5 Fuentes más comunes\n",
        "def limpiar_fuentes(x):\n",
        "    if pd.isna(x):\n",
        "        return []\n",
        "    if isinstance(x, str):\n",
        "        return [f.strip().upper() for f in re.split(r\"[,;|]+\", x) if f.strip()]\n",
        "    return []\n",
        "\n",
        "fuentes = (\n",
        "    base[\"fuentes_presentes\"]\n",
        "    .apply(limpiar_fuentes)\n",
        "    .explode()\n",
        "    .value_counts()\n",
        "    .reset_index()\n",
        ")\n",
        "fuentes.columns = [\"Fuente\", \"Total_Paros\"]\n",
        "\n",
        "# ---- 3.6 Rango de fechas analizado\n",
        "rango_fechas = {\n",
        "    \"Fecha más antigua\": base[\"fecha_inicio\"].min().strftime(\"%Y-%m-%d\"),\n",
        "    \"Fecha más reciente\": base[\"fecha_fin\"].max().strftime(\"%Y-%m-%d\"),\n",
        "    \"Total paros\": len(base)\n",
        "}\n",
        "\n",
        "# ================================================================\n",
        "# 4. EXPORTAR RESULTADOS\n",
        "# ================================================================\n",
        "with pd.ExcelWriter(\"Estadisticas_Paros_Unificados_2015.xlsx\", engine=\"openpyxl\") as writer:\n",
        "    paros_mes.to_excel(writer, sheet_name=\"Paros por Mes\", index=False)\n",
        "    tipos.to_excel(writer, sheet_name=\"Tipos de Movilización\", index=False)\n",
        "    orgs.to_excel(writer, sheet_name=\"Organizaciones\", index=False)\n",
        "    fuentes.to_excel(writer, sheet_name=\"Fuentes\", index=False)\n",
        "    pd.DataFrame([duracion_stats]).to_excel(writer, sheet_name=\"Duración\", index=False)\n",
        "    pd.DataFrame([rango_fechas]).to_excel(writer, sheet_name=\"Resumen General\", index=False)\n",
        "\n",
        "# ================================================================\n",
        "# 5. RESUMEN EN CONSOLA\n",
        "# ================================================================\n",
        "print(\"\\nResumen rápido:\")\n",
        "print(f\"Total de paros únicos: {len(base)}\")\n",
        "print(f\"Meses con paros: {base['mes'].nunique()}\")\n",
        "print(f\"Promedio de duración: {duracion_stats['Promedio (días)']} días\")\n",
        "print(f\"Rango de fechas: {rango_fechas['Fecha más antigua']} → {rango_fechas['Fecha más reciente']}\")\n",
        "\n",
        "print(\"\\nArchivo generado: Estadisticas_Paros_Unificados_2015.xlsx\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}